{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_4_3_visualisation.ipynb","provenance":[{"file_id":"1jBjznEyD7tESHhqZb22t_h5LaoAYoEXD","timestamp":1573651867682},{"file_id":"1O6kIGk8zzkRhofqvfUJ-gThN7V5DzJMp","timestamp":1571920454510},{"file_id":"1ovgdNE5im0_9Tldji46CJAFtbMPGxd-d","timestamp":1571049288677}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tFIQopoxdJb3","colab_type":"text"},"source":["# Visualisation: CE6003 Lab_4_3 (Optional)\n","\n","This exercise is based on the paper by Manhendran (2015) https://arxiv.org/pdf/1512.02017.pdf\n","\n","In this exercise the layers of the pre-trained VGG-16 network are to be visualised. \n","\n","Code has been included to load the model, display images and perform training.\n","\n","Make sure you select the Runtime -> Change Runtime type -> GPU for faster computation.\n","\n","For additionalinformation about visulisation & some nice images please view:  \n","https://distill.pub/2017/feature-visualization/?utm_campaign=Dynamically%20Typed&utm_medium=email&utm_source=Revue%20newsletter\n"," \n"]},{"cell_type":"code","metadata":{"id":"xHQqEZXHwFGU","colab_type":"code","outputId":"63d920fc-f3f1-4bc6-d10d-7de0dab72b50","executionInfo":{"status":"ok","timestamp":1573984168203,"user_tz":0,"elapsed":31850,"user":{"displayName":"Tony Scanlan","photoUrl":"","userId":"11380187689366571663"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["\n","!pip install -q tensorflow==2.0.0-rc\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","import time\n","import os\n","import numpy as np\n","import glob\n","import random\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","print(tf.__version__)\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 86.3MB 127kB/s \n","\u001b[K     |████████████████████████████████| 501kB 66.7MB/s \n","\u001b[K     |████████████████████████████████| 4.3MB 55.6MB/s \n","\u001b[?25h2.0.0-rc0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V1_sLQZMwKEl","colab_type":"text"},"source":["# Input parameters.\n","For this exercise Let's focus on visualising the layer block3_conv2. This is a good compromise between nice images (See Figure 17) and faster execution compared to deeper layers. (you can experiment with modifying the notebook to observe the effect of the scaling factor C between the loss and regularisers. The jitter term T can also be varied. Other layers can also be visualised, this will required different values of C, H, W, T and n_c)"]},{"cell_type":"code","metadata":{"id":"oeDUYC5ndNty","colab_type":"code","colab":{}},"source":["\n","\n","# Imagenet Rgb\n","imagenet_rgb_values = [123.68, 116.779, 103.939] # Preprocessing values for imagnet (no need to adjust)\n","\n","# Loss function parameters from paper\n","B =  80\n","V = B/6.5\n","C = 300 # Parmeter multiplies loss function value \n","Z = 98000 #Estimated Scaling Value for loss block3_conv2\n","alpha_val = 6\n","beta_val = 2\n","\n","# This selects the output layer we are going to visualise.\n","output_layer = ['block3_conv2'] \n","\n","H=40    #Input random noise image H\n","W=40    #Input random noise image W\n","\n","T= 1     # Jitter value\n","n_c = 5  # centre neuron (depends on the number of down sampling stages to layer interest)\n","\n","# Scalar for bounded range.\n","norm_r_alpha = 1/(H*W*(B**alpha_val))  # Alpha regulariser\n","# Scale for total variation regulation.\n","norm_r_beta  = 1/(H*W*(V**beta_val))  # Beta regulariser"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcfCwizMq133","colab_type":"code","colab":{}},"source":["# Helper function to output images at end/during optimisation\n","def display_image(image):\n","   \n","    title = ['Generated Image']\n","    plt.figure(figsize=(3,3))\n","    plt.title(title)\n","    plt.imshow(np.clip(image[0,:,:,:], 0, 255).astype(\"uint8\"),aspect=\"auto\")\n","    plt.axis('off')\n","    plt.show()\n","\n","    \n","def display_array(img_array,array_size):\n","    #unprocess model input and output\n","    #p_low_res = unprocess_image(low_res_input)\n","    #up_model_op = unprocess_image(model_op)\n","    n = array_size[0]\n","    m = array_size[1]\n","    plt.figure(figsize=(3*m,3*n))\n","    plt.title('Imaged Filters') \n","    ptr = 1\n","    for i in range(n):      \n","       for j in range(m):\n","          ax=plt.subplot(n,m,ptr)     \n","          plt.imshow(np.clip((img_array[ptr,:,:,:]+127.5), 0, 255).astype(\"uint8\"),aspect=\"auto\")  \n","          ptr = ptr+1\n","          plt.axis('off')\n","    plt.show()       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o0hiL7xxHMl","colab_type":"text"},"source":["# Model Loading\n","The in-built keras VGG16 model will be loaded and visualised.\n","Additional processing function is required (to subtract mean from image)"]},{"cell_type":"code","metadata":{"id":"dzcOGRZ1e7m7","colab_type":"code","outputId":"854a47a3-2fef-44ac-d217-245ba218c774","executionInfo":{"status":"ok","timestamp":1573984260123,"user_tz":0,"elapsed":37733,"user":{"displayName":"Tony Scanlan","photoUrl":"","userId":"11380187689366571663"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Load the Target model.\n","vgg16_model = tf.keras.applications.VGG16()\n","\n","# keras implementation of Vgg19 (loaded below) requires preprocessing of input images and RGB->BGR\n","def process_image(input_img):\n","    op_img =[]\n","    r = input_img[:,:, :, 0] - imagenet_rgb_values[0]\n","    g = input_img[:,:, :, 1] - imagenet_rgb_values[1]\n","    b = input_img[:,:, :, 2] - imagenet_rgb_values[2]\n","    op_img = tf.stack([b,g,r],axis = 3)\n","    return op_img\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467904/553467096 [==============================] - 34s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PRx-skwCtuVh","colab_type":"text"},"source":["A modified model \"red_model\" is created that has the same 224x224 input as Vgg16 but omits the layers after the layers of interest speeding up computation.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tUa5_Md_edix","colab_type":"code","outputId":"190c960c-577f-470d-d862-81142fabc645","executionInfo":{"status":"ok","timestamp":1573984284601,"user_tz":0,"elapsed":851,"user":{"displayName":"Tony Scanlan","photoUrl":"","userId":"11380187689366571663"}},"colab":{"base_uri":"https://localhost:8080/","height":497}},"source":["\n","\n","# Function to get layer output tensors & layer shapes.\n","def layer_data(layer_names):\n","    layer_shape = []  \n","    model_ops = []\n","    for layer in layer_names:\n","        model_ops.append(vgg16_model.get_layer(layer).output)\n","        layer_shape.append(vgg16_model.get_layer(layer).output_shape)\n","    return model_ops,layer_shape  \n","\n","# Define reduced model\n","model_ops,output_layer_shape = layer_data(output_layer) \n","red_model = tf.keras.Model(inputs = vgg16_model.input, outputs=model_ops ) \n","print(output_layer_shape)\n","\n","red_model.summary()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[(None, 56, 56, 256)]\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","=================================================================\n","Total params: 1,145,408\n","Trainable params: 1,145,408\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0ClfFjjyro1O","colab_type":"text"},"source":["#Loss function & Regularisation:\n","\n","In order to perform visualisation the correct loss function and regularisers (bounded range and bounded variation) from section 3 in the paper are implemented.  \n","\n","Note that jitter can be implemented by a tensorflow function in the training step.\n","\n"]},{"cell_type":"code","metadata":{"id":"CCwlx7mjevXG","colab_type":"code","colab":{}},"source":["# Loss function due to activation maximisation.\n","def feat_loss(output_features):\n","    #loss = tf.keras.backend.mean(model_output[:, :, :, filter_index])\n","    loss = -(1/Z)*tf.math.reduce_sum(output_features[:, n_c, n_c, filter_index])\n","    return C*loss\n","    \n","    \n","# Add bouded range regulariser.\n","def bounded_rng_reg(img):\n","    #square all values\n","    x = tf.math.square(img)\n","    #sum over channels\n","    x = tf.keras.backend.sum(x,axis=3)\n","    #raise to a power\n","    x = tf.keras.backend.pow(x,alpha_val/2)\n","    #sum over remaining channels\n","    x=tf.math.reduce_sum(x)\n","    #apply scalar function.\n","    x = norm_r_alpha*x\n","    return x\n","  \n","  \n","# conditional value to bound the output  \n","#  Return on a per element basis, so that are gradients for each image value.\n","def cond_value(img):\n","    x = tf.math.square(img)\n","    x = tf.keras.backend.sum(x,axis=3)\n","    y = tf.keras.backend.pow(x,1/2)  \n","    return y\n","  \n","  \n","# Differences total variation loss\n","def high_pass_x_y(image):\n","  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n","  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n","  # Pad zeros along difference dimension for von neumann boundary. (dim reduced)\n","  pad_x = [[0,0],[0, 0], [0, 1],[0,0]]\n","  pad_y = [[0,0],[0, 1], [0, 0],[0,0]]      \n","  x_var = tf.pad(x_var, pad_x, 'CONSTANT', constant_values=0)\n","  y_var = tf.pad(y_var, pad_y, 'CONSTANT', constant_values=0)       \n","  return x_var, y_var\n","\n","\n","# Total variation loss\n","def TV_loss(image):\n","  x_deltas, y_deltas = high_pass_x_y(image)\n","  x = (x_deltas**2) + (y_deltas**2)\n","  x=(tf.keras.backend.pow(x,beta_val/2))\n","  x=tf.reduce_sum(x)\n","  tv_loss = norm_r_beta*x\n","  return tv_loss\n","\n","\n","\n","# Combined losses\n","def total_loss(output_features,img):\n","    l1 =  feat_loss(output_features)\n","    l1_bnd =   bounded_rng_reg(img)\n","    tv_loss = TV_loss(img)\n","    t_loss =  l1+l1_bnd+tv_loss\n","    return l1,l1_bnd,t_loss,tv_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ad5KpKTiBLUF","colab_type":"text"},"source":["#Training Step\n","Note that the paper defines a specific stocasitic gradient descent algorithm, we can use the built in ADAM optimiser for convience.\n","Please note how jitter can be implemented (note it can be disabled, set jitter_on = False and T=0 in parameter list). it is recommended to not use jitter until you are satisfied you are getting images. The jitter slightly improves quallity. "]},{"cell_type":"code","metadata":{"id":"xEmPoxdke263","colab_type":"code","colab":{}},"source":["jitter_on = True\n","# optimiser for gradients\n","adam_optimiser = tf.keras.optimizers.Adam(0.1, beta_1=0.09, beta_2=0.99, epsilon=1e-7)\n","\n","\n","# Traing step function\n","def train_step(rnd_img_data):\n","  with tf.GradientTape() as gen_tape:\n","    \n","    gen_tape.watch(rnd_img_data)  \n","    if jitter_on == True:   \n","       inp_img = tf.image.random_crop(rnd_img_data,(1,H,W,3))\n","    else:\n","        inp_img = rnd_img_data\n","    op_features=red_model(process_image(inp_img+127.0))\n","    nx = np.max(cond_value(rnd_img_data))\n","    l1,l1_bnd,t_loss,tv_loss = total_loss(op_features,rnd_img_data)\n"," \n","  img_grads = gen_tape.gradient(t_loss,rnd_img_data)\n","  adam_optimiser.apply_gradients([(img_grads,rnd_img_data)])\n","  return l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3yRTVLf1yWq","colab_type":"text"},"source":["# Complete Training\n","The two cells below contain two training loops. Use the first to debug any changes you make to the notebook as it will display images and loss values perodically.\n","Use the second to iterate over a few filters and plot the images similar to Figure 17 in the paper.\n","\n","Note that before iteration a random noise image is applied to the input. The mean and standard deviation of this image must be set before training.\n","\n","As metioned in the paper the value of C must be tuned to balance the loss against the regularisers. It may take some experimentation to find a good value of C if you have have made changes to the notebook and are trying to visualise other layers. "]},{"cell_type":"code","metadata":{"id":"YtjlRw7KX4Qs","colab_type":"code","colab":{}},"source":["# Determine what image maximally stimulates a neuron\n","\n","iterations = 2000\n","filter_index =1\n","\n","# mean and stdev for input image noise vector.\n","mean_val = 0\n","std_val = 20.0\n","\n","# initialise noise vector\n","rnd_img_data = tf.Variable( tf.random.normal([1, H+T,W+T,3],mean=mean_val,stddev=std_val,name=\"input_img_data\"))\n","rnd_img_data = tf.cast(rnd_img_data, dtype='float32')\n","\n","for iterations in range(iterations):\n","    l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss = train_step(rnd_img_data)\n","    if iterations % 100 == 0:\n","       print('Iteration Number =',iterations)\n","       print('Total loss =',t_loss)\n","       print('Feature Maximisation =',l1)\n","       print('Bounded regularisation =',l1_bnd)\n","       print('Total Variation =',tv_loss)\n","       print(nx)\n","       #print('Total Variation Loss=',tv_loss)\n","       display_image(rnd_img_data+127.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zlj5UcSC2mTw","colab_type":"code","colab":{}},"source":["iterations = 2000\n","filter_list =[1,2,3,4,5,6]\n","#filter_list = list(range(0, 25))\n","\n","\n","mean_val = 0\n","std_val = 20.0\n","\n","layer_images =  tf.Variable(np.zeros([1,H+T,W+T,3]))\n","layer_images = tf.cast(layer_images, dtype='float32')\n","\n","# Code to produce multiple filter images in each layer.\n","for filter_index in filter_list:\n","    # reset random vector\n","    rnd_img_data = tf.Variable( tf.random.normal([1, H+T,W+T,3],mean=mean_val,stddev=std_val,name=\"input_img_data\"))\n","    rnd_img_data = tf.cast(rnd_img_data, dtype='float32')\n","    \n","    for iterations in range(iterations):\n","        l1,l1_bnd,t_loss,rnd_img_data,nx,tv_loss = train_step(rnd_img_data)\n","        if iterations % 100 == 0:\n","          print('Iteration Number =',iterations)\n","    layer_images = tf.concat([layer_images, rnd_img_data],0) \n","    display_image(rnd_img_data+127.0)\n","# reshape into grid and display.    \n","display_array(layer_images,(2,3))\n","#display_array(layer_images,(5,5))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LN8KTQjs2q7Y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}