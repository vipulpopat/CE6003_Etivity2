{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_4_2_ResNet (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMd1Rc3eDwBP",
        "colab_type": "text"
      },
      "source": [
        "# ResNet20 on CIFAR-10 Dataset\n",
        "\n",
        "In this lab, we will gain some practical experience using ResNet, a Deep Convolutional Neural Network (CNN) approach to object classification. Convolutional Neural Networks build up layers of convolutions, transforming an input image and distilling it down until they start recognizing composite features, with deeper layers of convolutions recognizing increasingly complex patterns.\n",
        "\n",
        "ResNet was invented by Researchers at Microsoft Research Asia (Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun), and it won first place in both ILSVRC and MS COCO competitions in 2015.\n",
        "\n",
        "A great visual presentation from Kaiming He describing ResNet is available at:\n",
        "\n",
        "http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf.\n",
        "\n",
        "Whereas AlexNet (ILSVRC 2012 winner) had 8 layers, VGG (ILSRVC 2014) had 19 layers, and GoogleNet (ILSRVC 2014) had 22 layers, ResNet demonstrated a trainable network with a whopping 152 layers! This was quite a technical achievement at the time. ResNet managed this by resolving a phenomenon that had to this point been observed across many different datasets - namely, that deeper networks experienced higher training and test error than their shallower counterparts.\n",
        "\n",
        "By contrast, deep ResNets can be easily trained, and in fact have lower training error and lower test error than comparable, shallower, networks. Furthermore, ResNets can learn better features which are more transferrable to networks used for other tasks. These better features lead to improved performance on object detection and segmentation tasks when using ResNet as the base classifier.\n",
        "\n",
        "ResNets achieve this through the use of a technique called Residual Deep Learning. This is a technique that involves passing through the main signal of the input data, so that the network ultimately \"learns\" on just the residual portions that differ between layers. This has proven, in practice, to allow the training of much deeper networks by avoiding issues that plague gradient descent on larger networks. These cells bypass convolution layers and then come back in later before rectified linear unit (ReLU).\n",
        "\n",
        "For this task, we have chosen ResNet20 as our trained model architecture. The code in this lab is based on https://keras.io/examples/cifar10_resnet/, and in particular the ResNet20 v1 model\n",
        "\n",
        "We will use the Keras API for TensorFlow 2 in this example, along with NumPy, the Python library for multi-dimensional numerical computing.\n",
        "\n",
        "Let's now import TensorFlow 2 and NumPy into our Python runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCZcWwiaDOGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39549266-1161-4c08-ff05-2b3f248d715c"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odR82ILcVxX",
        "colab_type": "text"
      },
      "source": [
        "To ensure these labs run as fast as possible, from the menu above select **Edit > Notebook settings** or **Runtime > Change runtime type** and select GPU as the Hardware accelerator option.\n",
        "\n",
        "Let's test that we are running using the GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf087-5LcYXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a61d7ae7-cda5-4037-c7e0-98250eb1b875"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng8h2DMDcaYK",
        "colab_type": "text"
      },
      "source": [
        "If this outputs '', then we are running on CPU only. If it outputs something like '/device:GPU:0' then we are running on GPU. If you see something like ...\n",
        "\n",
        "    Failed to assign a backend\n",
        "    No backend with GPU available. WOuld you like to use a runtime with no accelerator?\n",
        "\n",
        "This suggests that many other users have all the GPU resources on colab occupied at the moment, so perhaps try later or try using with the TPU instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1AqzPcvPIMZ",
        "colab_type": "text"
      },
      "source": [
        "# Preparing our Dataset\n",
        "\n",
        "In this tutorial, we will be using the CIFAR-10 dataset (http://www.cs.toronto.edu/~kriz/cifar.html). \n",
        "CIFAR-10 is a popular dataset for image classification, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. \n",
        "There are 50,000 training images and 10,000 test images. The 10 classes are: \n",
        "    \n",
        " * airplane, \n",
        " * automobile, \n",
        " * bird, \n",
        " * cat, \n",
        " * deer,\n",
        " * dog,\n",
        " * frog,\n",
        " * horse,\n",
        " * ship, and \n",
        " * truck.\n",
        "\n",
        "The Keras interface to TensorFlow 2.x has specific helper functions to make our lives easy when downloading and using the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBHnROl5fL2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4eb18bf8-17b1-4788-faa5-69e62567ff05"
      },
      "source": [
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_test_orig = x_test\n",
        "y_test_orig = y_test\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Subtracting the pixel mean improves accuracy\n",
        "x_train_mean = np.mean(x_train, axis=0)\n",
        "x_train -= x_train_mean\n",
        "x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDl7tySEfvFh",
        "colab_type": "text"
      },
      "source": [
        "We need to setup a few parameters we'll use later. We need to define the size of our mini-batch, the number of epochs we plan to train for, and the number of classes we are training for - in CIFAR-10, there are 10 separate labelled classes of object.\n",
        "\n",
        "We'll set the depth of our network to 20, indicating we are building a ResNet20 classifier (the smallest model in the ResNet family).\n",
        "\n",
        "Finally, we'll convert our labels from numeric class vectors to binary class matrices. Thsi is to ensure that training runs successfully. This will convert a class label of '3', for example, indicating 'cat', to `[ 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 ]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpL6ksEGflIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "\n",
        "depth = 20 \n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKjhi09Kfwem",
        "colab_type": "text"
      },
      "source": [
        "# The ResNet Block Layer\n",
        "\n",
        "## Basic Building Blocks of CNN\n",
        "\n",
        "*Convolutional Neural Networks* (also known as *CNNs*, or *ConvNets*) typically stack convolutional layers, rectified linear unit (ReLU) activation layers, pooling layers, and finally fully connected layers. \n",
        "\n",
        "ResNet changes this slightly, and introduces a new trend towards smaller filters and deeper architectures, with fewer pooling and fully connected layers.\n",
        "\n",
        "We need to create some basic building blocks for the ResNet model. Let's take a look at the Resnet *residual block*, which we will use to create our the full ResNet20 network architecture.\n",
        "\n",
        "![The Residual Block](https://github.com/EmdaloTechnologies/CE6003/blob/master/images/lab4/residual_block_color.png?raw=1 \"The Residual Block\")\n",
        "\n",
        "As you can see, the residual block consists of a stacking of convolutional, batch normalization, and rectified linear (ReLU) activation layers. This residual block is our basic building block for ResNet.  We explained how it works in our video lectures for this module, and will recap on this shortly.  \n",
        "\n",
        "So we now have a function to create a convolutional weights layer with back normalization at the end, and one which also has a rectified linear unit (ReLU) after it.  We're next going to combine both of these into the fundamental building block of ResNet, the *Residual Block*.\n",
        "\n",
        "## The Residual Block and Skip Connections\n",
        "\n",
        "The following function will create the residual block, and a version of the residual block that also adds in input passed through a single convolutional layer and batch norm.\n",
        "\n",
        "![resnet_basic_blocks](https://github.com/EmdaloTechnologies/CE6003/blob/master/images/lab4/resnet_basic_blocks.png?raw=1 \"ResNet basic building blocks\")\n",
        "\n",
        "We can see the skip connection implemented above with the addition of x plus either the input layer or y, i.e.: \n",
        "\n",
        "$ x = x + y$\n",
        "\n",
        "\n",
        "Now we need to create some code to stack layers together, increasing the number of filters and decreasing the spatial size of the data as it progresses through the network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or_1E25YFrdA",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Here, we need to create a `resnet_layer` function with the following function prototype, to complete this lab:\n",
        "\n",
        "    def resnet_layer(inputs, num_filters=16, kernel_size=3,\n",
        "                     strides=1, activation='relu', batch_normalization=True,\n",
        "                     conv_first=True):\n",
        "\n",
        "\n",
        "The function needs to create a stack of Keras layers. specifically Conv2D, BatchNormalization, and Activation:\n",
        "\n",
        " * if `conv_first` is True, then the first layer needs to be a Conv2D layer using the specified number of filters, kernel size, stride;\n",
        "     * You can use `same` for padding, `he_normal` for the kernel initializer, and `l2(1e-4)` for the kernel regularizer.\n",
        " * if `batch_normalization`is True, then add a BatchNormalization layer to the Conv2D layer\n",
        " * if activation is not `None`, then add an Activation layer to the BatchNormalizationLayer\n",
        "\n",
        "Then return the stack.\n",
        "\n",
        "You can find out more about the Keras Layers API at https://keras.io/layers/about-keras-layers/ :\n",
        "\n",
        " * Convolutional Layers are described at https://keras.io/layers/convolutional/\n",
        " * Normalization layers are described at https://keras.io/layers/normalization/\n",
        " * Activation layers are described at https://keras.io/layers/advanced-activations/\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFwDIngPfrww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercise: Your Code Here\n",
        "#notes to self - this is effectively code to build a repeatable block of layers where it's possible to specify the inputs to the layers and the parameters to the layers.  \n",
        "#pseudo code\n",
        "#if conv_first = True then trigger code for a 2D convolutional layer that uses kernel, filters and stride outlined in the parameters.\n",
        "#if batch_norm & conv_first are true then trigger conv layer then batch norm layer\n",
        "#if activation function specifies none, we do not add a fully connected layer. \n",
        "# it must be possible for the matrix to pass through this layer unimpacted if conv_first = False, batch_norm = False and activation = None. \n",
        "# it must be possible any of them to run on their own or any combination of the two to run. \n",
        "\n",
        "def resnet_layer(inputs, num_filters=16, kernel_size=3,\n",
        "                 strides=1, activation='relu', batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    #\n",
        "    # Put your code here\n",
        "    conv_layer = keras.layers.Conv2D(num_filters, kernel_size, strides, padding='same', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, \n",
        "                                     kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=l2(0.0001), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
        "    bn_layer = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n",
        "                                               gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
        "                                               beta_constraint=None, gamma_constraint=None)\n",
        "    act_layer = 0 #tbc on how to trigger an activation layer taking the activation function\n",
        "\n",
        "    \n",
        "    if conv_first == False and batch_normalization == False and activation is not 'none':\n",
        "      inputs = x\n",
        "    else:\n",
        "       x =0 #holding code\n",
        "    #\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejkanHEVfyNO",
        "colab_type": "text"
      },
      "source": [
        "# ResNet20 Network Architecture\n",
        "\n",
        "With these functions, we now have enough to plumb together the architecture of ResNet20.  We are using the network architecture of ResNet defined in the [ResNet paper](http://arxiv.org/abs/1512.03385), also known as ResNet v1.\n",
        " \n",
        "The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [Batch normalization](https://arxiv.org/abs/1502.03167) is applied everywhere except the last fully-connected layer.\n",
        "\n",
        "For now, let's look at how we stack our building blocks above blocks together to form the ResNet20 model.  At the beginning, we have a single convolutional weights layer, followed by a batch normalization layer and a rectified linear unit.  Then we have our residual units, and we end with an average pooling layer and a fully-connected (also called a *dense*) layer which gives us our class predictions.\n",
        "\n",
        "In the deeper ResNet Models, bottleneck blocks are introduced for computational considerations - i.e., they serve to reduce the dimensionality of the network, which will help prevent overfitting and improve inference and training time. In all ResNet variants, dimensionality is reduced at the end by the global average pooling layer. This layer further helps minimize the chances of overfitting by reducing the number of total parameters within the model (i.e., reducing the spatial dimensions of 3D tensors). See https://arxiv.org/pdf/1312.4400.pdf (especially section 3.2) for more details on global average pooling. ResNet doesn't require the use of dropout because it uses global average pooling instead to reduce the dimensionality before the dense fully-connected layer.\n",
        "\n",
        "ResNet20 doesn't use bottlenecks, so we won't discuss this further in this lab.\n",
        "\n",
        "Here is the full ResNet20 network architecture:\n",
        "\n",
        "![ResNet20 model](https://github.com/EmdaloTechnologies/CE6003/blob/master/images/lab4/resnet20_model_color.png?raw=1 \"ResNet20 Network Architecture\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwPACVhkf0uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n",
        "            y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x, num_filters=num_filters,\n",
        "                                 kernel_size=1, strides=strides,\n",
        "                                 activation=None, batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "661APuAef2p4",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined our model creation function, we'll instantiate a model, and compile it.  Remember that compiling, in Keras, configures the model for training - it defines the loss function, the optimizer and metrics. \n",
        "\n",
        "Compiling does not affect or modify the weights, and you can compile a model as often as you want without affecting pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCg5D0sf6E1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "05183991-38b2-4a23-dc0f-d8727b94de4e"
      },
      "source": [
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Uncomment the following line to plot the model visually - warning, even for ResNet20, it is quite large!\n",
        "# keras.utils.plot_model(model, dpi=48)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9d8194db0503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Uncomment the following line to plot the model visually - warning, even for ResNet20, it is quite large!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-765a7b9ca05a>\u001b[0m in \u001b[0;36mresnet_v1\u001b[0;34m(input_shape, depth, num_classes)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                  \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                  activation=None, batch_normalization=False)\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnum_filters\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/layers/merge.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(inputs, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m   \"\"\"\n\u001b[0;32m--> 587\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m                        \u001b[0;34m'on a list of at least 2 inputs. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                        'Got ' + str(len(input_shape)) + ' inputs.')\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/layers/merge.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m                        \u001b[0;34m'on a list of at least 2 inputs. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                        'Got ' + str(len(input_shape)) + ' inputs.')\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzfMkJAUgT5l",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now that we have our model created, it is time to perform training to fit our model weights to our training data. Conceptually, the flow is as follows - we evaluate the error for the class prediction of a test image with a known label using our current set of weights, and then we use back-propagation to adjust our weights slightly to reduce this error. We do this for all the examples in our training set. This is one *epoch*. Then we repeat until either a fixed numer of epochs or, more likely, until our error has reduced to a suitable small number.\n",
        "\n",
        "In practice, we are splitting our test data in smaller chunks called mini-batches. When all of the training data has been processed once across a number of mini-batches, this is will be an *epoch*.\n",
        "\n",
        "Using mini-batches has a number of advantages over single batch for training. Firstly, it uses less peak memory and therefore allows much larger training sets than might otherwise be feasible.  Secondly, it acts as a source of noise.  This small level of noise ensures that you escape being trapped in any local minima. Using minibatching does require that error information is accumulated across the entire batch before doing backpropagation.\n",
        "\n",
        "![Training Workflow](https://github.com/EmdaloTechnologies/CE6003/blob/master/images/lab4/cnn_training.gif?raw=1 \"Training Workflow\")\n",
        "\n",
        "We will attempt to train for 20 epochs. This will take about 15-20 minutes on Colab. If you want, you can reduce the `epochs = 20` line in the code to a smaller number, to train quicker, perhaps 5. Or increase it to 50, to see the difference training for longer makes to the training curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9LcX-rwPbxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "    validation_data=(x_test, y_test), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FZ6cNvVf_bz",
        "colab_type": "text"
      },
      "source": [
        "Now that we have finished training, let's again take a look at our training history and plot our training curves.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHas5bv9gAlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# this next line is just Jupyter Notebook magic to plot inline in the webpage...\n",
        "%matplotlib inline \n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e16_Awsoeg8",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll take a look at our predictions. In this case, we're going to cheat and reuse our x_test, pretending it is unseen data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgFicmYTsun7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9RU4LvQonvp",
        "colab_type": "text"
      },
      "source": [
        "We're going to define a helper function, `find_index_of_max_value_in_array`, to locate the index of the element in an array with the highest numerical value. We'll use thi later to find the most likely class based on our set of probability scores for a particular inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amn-S6cRuuFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_index_of_max_value_in_array(input_array, input_index):\n",
        "  score_index = np.where(input_array[input_index] == np.amax(input_array[input_index]))\n",
        "  score_index = np.squeeze(score_index, axis=(0,1)).item()\n",
        "  return score_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dutZW12o5sG",
        "colab_type": "text"
      },
      "source": [
        "The following code block is going to plot our predictions in a table - you don't need to worry about how it works exactly -- instead, focus on the table which shows the test images, the predicted label, the actual ground truth label in each case, and the full set of prediction scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-SCWtTZgysK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# This code is to allow us to inline our images for display in a table...\n",
        "#\n",
        "from IPython.display import HTML, display\n",
        "import urllib\n",
        "import pandas\n",
        "import PIL.Image\n",
        "import io\n",
        "import base64\n",
        "import cv2\n",
        "\n",
        "pandas.set_option('display.max_colwidth', -1)\n",
        "\n",
        "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# this function will take an index for a test set image, and convert it to base64 encoding\n",
        "def encode_image_as_base64(xtest_index):\n",
        "  temp  = io.BytesIO()\n",
        "  image = PIL.Image.fromarray(x_test_orig[xtest_index], 'RGB')\n",
        "  image.save(temp, format=\"png\")\n",
        "  return base64.b64encode(temp.getvalue()).decode()\n",
        "\n",
        "# this function acts as a formatting, mapping a test set image number to that image, in base64 format\n",
        "def map_image_data_to_inline_image(xtest_index):\n",
        "  return '<img src=\"data:image/jpeg;base64,{}\" width=64 height=64 alt=IMAGE />'.format(encode_image_as_base64(xtest_index))\n",
        "  \n",
        "#\n",
        "# The following code will build and display our table of images, ground truth, prediction, and prediction probability scores\n",
        "#\n",
        "\n",
        "pred_count=0\n",
        "num_images = 30\n",
        "precision = \"{:.4f}\"\n",
        "correct_count = 0\n",
        "\n",
        "my_results = []\n",
        "\n",
        "for prediction in predictions:\n",
        "  true_label = y_test_orig[pred_count][0]\n",
        "  predicted_label = find_index_of_max_value_in_array(predictions, pred_count);\n",
        "\n",
        "  my_results.append({\"image_data\":   pred_count, \n",
        "                      \"truth\":       labels[true_label],\n",
        "                      \"prediction\":  labels[predicted_label],\n",
        "                      labels[0]:     precision.format(prediction[0]),\n",
        "                      labels[1]:     precision.format(prediction[1]),\n",
        "                      labels[2]:     precision.format(prediction[2]),\n",
        "                      labels[3]:     precision.format(prediction[3]),\n",
        "                      labels[4]:     precision.format(prediction[4]),\n",
        "                      labels[5]:     precision.format(prediction[5]),\n",
        "                      labels[6]:     precision.format(prediction[6]),\n",
        "                      labels[7]:     precision.format(prediction[7]),\n",
        "                      labels[8]:     precision.format(prediction[8]),\n",
        "                      labels[9]:     precision.format(prediction[9]) })\n",
        "\n",
        "  pred_count += 1;\n",
        "\n",
        "  if predicted_label == true_label:\n",
        "    correct_count += 1\n",
        "\n",
        "  if pred_count >= num_images:\n",
        "    break\n",
        "\n",
        "  if pred_count % 100 == 0:\n",
        "    print(\"Processed {0} samples ({1:.2%} correct)\".format(pred_count, (float(correct_count) / pred_count)))\n",
        "\n",
        "# Let's format our results table nicely for Jupyter Notebook\n",
        "# we'll use a Pandas DataFrame, and re-order the columns slightly so that the image,\n",
        "# ground truth label and best prediction are displayed first...\n",
        "df = pandas.DataFrame(my_results)\n",
        "cols = list(df.columns.values)\n",
        "cols.pop(cols.index('image_data'))\n",
        "cols.pop(cols.index('truth'))\n",
        "cols.pop(cols.index('prediction'))\n",
        "df = df[['image_data'] + ['truth']+ ['prediction'] + cols]\n",
        "\n",
        "\n",
        "# and finally, render it as a nice HTML table below\n",
        "display(HTML(df.to_html(escape=False, formatters=dict(image_data=map_image_data_to_inline_image))))\n",
        "            \n",
        "print (\"{0} of {1} prediction were correct\".format(correct_count, pred_count))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox7oPgKtr_NW",
        "colab_type": "text"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "1. Consider changing `num_epochs` from 20 to 50, and repeating the lab. What happens to the training curves now?\n",
        "\n",
        "**Note:** Google Colab notebooks have an idle timeout of 90 minutes and absolute timeout of 12 hours. This means, if user does not interact with a running Google Colab notebook for more than 90 minutes, its instance is automatically terminated. Also, maximum lifetime of a Colab instance is 12 hours.  As each epoch of this lab on a GPU takes between 30 seconds and 1 minute to train (presumably depending on the load on Colab), if you want to increase the number of epochs to be 90 or more, then remember to interact with the notebook somehow at least more than every 90 minutes.\n",
        "\n",
        "2. Consider changing the runtime type from GPU to CPU and running a single epoch of training - how much slower is it to train on the CPU versus the GPU?\n",
        "\n",
        "3. This lab is derived from an a Keras example project. The example is more sophisticated in that it supports both ResNet v1 and ResNet v2 models. It also includes a very good example of real-time data augmentation when training. It is definitely worth working thorugh the code of the example at https://keras.io/examples/cifar10_resnet/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diL3ViCfIrfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}